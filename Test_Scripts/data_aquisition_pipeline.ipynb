{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Solar Flare prediction dataset pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a replication of the dataset used by [Liu et al.](https://web.njit.edu/~wangj/LSTMpredict/).\n",
    "\n",
    "The data comes from 2 sources:\n",
    "\n",
    "1. Flare data from the GOES flare catalog at NOAA, which can be accessed with the sunpy.instr.goes.get_event_list() function.\n",
    " This tells us if an active region produced a flare or not.\n",
    "2. Active region data from the Solar Dynamics Observatory's Heliosesmic and Magnetic Imager instrument, which can be accessed from the JSOC database via a JSON API.\n",
    "This gives us the features characterizing each active region.\n",
    "\n",
    "We ascribe each Active Region (AR) to one of two classes:\n",
    "\n",
    "1. The positive class contains flaring active regions that will produce\n",
    "flare >M5.0 in the next 24hours.\n",
    "2. The negative class contains flaring active regions that will **not**\n",
    "produce flare >M5.0 in the next 24hours.\n",
    "\n",
    "First, some imports.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "from datetime import datetime as dt_obj\n",
    "from datetime import timedelta\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sunpy.time import TimeRange\n",
    "from sunpy.net import hek\n",
    "from astropy.time import Time\n",
    "import sunpy.instr.goes\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import os\n",
    "import drms\n",
    "pd.set_option('display.max_rows', 500)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Get flare list\n",
    "\n",
    "We get the entire GOES flare catalog at NOAA.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbed all the GOES data; there are 11986 events.\n"
     ]
    }
   ],
   "source": [
    "# Grab all the data from the GOES database\n",
    "t_start = \"2010-05-01\"\n",
    "t_end = \"2018-05-30\"\n",
    "time_range = TimeRange(t_start, t_end)\n",
    "if os.path.exists(\"../Data/GOES/all_flares_list.csv\"):\n",
    "    listofresults = pd.read_csv('../Data/GOES/all_flares_list.csv').drop\\\n",
    "        (columns=\"Unnamed: 0\")\n",
    "else:\n",
    "    listofresults = sunpy.instr.goes.get_goes_event_list(time_range, 'B1')\n",
    "    # Remove all events without NOAA number\n",
    "    listofresults = listofresults[listofresults['noaa_active_region'] != 0]\n",
    "    # save to csv\n",
    "    pd.DataFrame(listofresults).to_csv('../Data/GOES/all_flares_list.csv')\n",
    "    listofresults = pd.DataFrame(listofresults)\n",
    "\n",
    "print('Grabbed all the GOES data; there are', len(listofresults), 'events.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the ```times``` in the ```listofresults``` dataframe from a string\n",
    "into a datetime object:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def parse_tai_string(tstr):\n",
    "    year = int(tstr[:4])\n",
    "    month = int(tstr[5:7])\n",
    "    day = int(tstr[8:10])\n",
    "    hour = int(tstr[11:13])\n",
    "    minute = int(tstr[14:16])\n",
    "    return dt_obj(year, month, day, hour, minute)\n",
    "\n",
    "listofresults['start_time'] = listofresults['start_time'].apply(parse_tai_string)\n",
    "listofresults['peak_time'] = listofresults['peak_time'].apply(parse_tai_string)\n",
    "listofresults['end_time'] = listofresults['end_time'].apply(parse_tai_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's query the JSOC database to see if there are active region parameters at the time of the flare.\n",
    "First read the following file to map NOAA active region numbers to HARPNUMs (a HARP, or an HMI Active Region Patch, is the preferred numbering system for the HMI active regions as they appear in the magnetic field data before NOAA observes them in white light):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "HARP_NOAA_list = pd.read_csv(\n",
    "    'http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt', sep=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's determine at which time we'd like to predict CMEs. In general,\n",
    "many people try to predict a CME either 24 or 48 hours before it happens.\n",
    "We can report both in this study by setting a variable called ```timedelayvariable```:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "timedelayvariable = 24"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we want the list of all the flares with its corresponding: label,\n",
    "flare_class, timestep, NOAA and HARP number.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# first and peak result of AR\n",
    "first = listofresults.groupby('noaa_active_region').nth(0)['start_time']\n",
    "last = listofresults.groupby('noaa_active_region').nth(-1)['end_time']\n",
    "\n",
    "# sample at 1 hour cadence between start and end time of AR\n",
    "t_range_per_AR = [[pd.date_range(first.iloc[i], last.iloc[i],\n",
    "    freq='1h'), first.index[i]] for i in range(first.shape[0])]\n",
    "\n",
    "flare_list = pd.DataFrame()\n",
    "# make dataframe from ranges.\n",
    "for timesteps in t_range_per_AR:\n",
    "    timesteps_df = pd.DataFrame([timesteps[0]])\n",
    "    AR = pd.DataFrame([timesteps[1]]* len(timesteps[0]))\n",
    "    ar_time = pd.concat([timesteps_df, AR.T])\n",
    "    flare_list = pd.concat([flare_list, ar_time.T],\n",
    "                                  ignore_index=True)\n",
    "\n",
    "flare_list.columns = ['timestamp', 'NOAA']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have the full length of time. We need to assign each time and AR with\n",
    "its accompanying flare\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# make dataframe of per active region\n",
    "# flare_times = pd.DataFrame()\n",
    "# for i in range (listofresults.shape[0]):\n",
    "#     t_range = pd.date_range(listofresults['start_time'][i],\n",
    "#                          listofresults['peak_time'][i], freq='1h').to_frame()\n",
    "#     flare_class = listofresults['goes_class'][i]\n",
    "#     for j in range(len(t_range)):\n",
    "#         timestep = t_range.iloc[j]\n",
    "#         flare_times = pd.concat([flare_times, pd.DataFrame([flare_class,\n",
    "#                                                             timestep[0]\n",
    "#                                                            ]).T ])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Get SHARP data\n",
    "\n",
    "Now we can grab the SDO data from the JSOC database by executing the JSON queries.\n",
    "We are selecting data that satisfies several criteria:\n",
    "The data has to be [1] disambiguated with a version of the disambiguation module greater than 1.1,\n",
    " [2] taken while the orbital velocity of the spacecraft is less than 3500 m/s,\n",
    " [3] of a high quality, and\n",
    " [4] within 70 degrees of central meridian.\n",
    " If the data pass all these tests, they are stuffed into one of two lists:\n",
    " one for the positive class (called pos_flare_data)\n",
    " and one for the negative class (called neg_flare_data)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now we prepare the data tobe fed into function\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "minimum_class_label = ['M5', 'M6', 'M7', 'M8', 'M9', 'X']\n",
    "listofactiveregions = list(listofresults['noaa_active_region'].unique())\n",
    "listofgoesclasses = list(listofresults['goes_class'].values.flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_the_jsoc_data(event_count):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_count: number of events\n",
    "                 int\n",
    "\n",
    "    t_rec:       list of times, one associated with each event in event_count\n",
    "                 list of strings in JSOC format ('%Y.%m.%d_%H:%M_TAI')\n",
    "\n",
    "    \"\"\"\n",
    "    from astropy.time import Time\n",
    "    start_date = drms.to_datetime(t_start).strftime('%Y.%m.%d_%H:%M_TAI')\n",
    "    end_date = drms.to_datetime(t_end).strftime('%Y.%m.%d_%H:%M_TAI')\n",
    "    series_sharp = 'hmi.sharp_cea_720s'\n",
    "    series_lorentz = 'cgem.lorentz'\n",
    "    ids = ['T_REC','NOAA_AR', 'HARPNUM', 'CRVAL1','CRVAL2', 'CRLN_OBS',\n",
    "           'CRLT_OBS', 'LAT_FWT', 'LON_FWT']\n",
    "    sharps = ['USFLUX', 'MEANGBT',\n",
    "              'MEANJZH', 'MEANPOT', 'SHRGT45',\n",
    "              'TOTUSJH', 'MEANGBH','MEANALP','MEANGAM','MEANGBZ','MEANJZD',\n",
    "              'TOTUSJZ','SAVNCPP', 'TOTPOT','MEANSHR','AREA_ACR','R_VALUE',\n",
    "              'ABSNJZH']\n",
    "    lorentzs = ['TOTFX','TOTFY','TOTFZ','EPSX','EPSY','EPSZ']\n",
    "    conditions = '(CODEVER7 !~ \"1.1\") and (abs(OBS_VR)< 3500) and (QUALITY<65536)'\n",
    "    conditions_lor = '(abs(OBS_VR)< 3500) and (QUALITY<65536)'\n",
    "    c = drms.Client()\n",
    "    data_jsoc = pd.DataFrame()\n",
    "\n",
    "    for i in range(event_count):\n",
    "\n",
    "        print(\"=====\", i, \"=====\")\n",
    "        # next match NOAA_ARS to HARPNUM\n",
    "        idx = HARP_NOAA_list[HARP_NOAA_list['NOAA_ARS'].str.contains(\n",
    "            str(int(listofactiveregions[i])))]\n",
    "\n",
    "        # if there's no HARPNUM, quit\n",
    "        if (idx.empty == True):\n",
    "            print('skip: there are no matching HARPNUMs for',\n",
    "                  str(int(listofactiveregions[i])))\n",
    "            continue\n",
    "\n",
    "        harpnum = idx.HARPNUM.values[0]\n",
    "        # query jsoc database for sharp data\n",
    "        data_sharp = c.query('%s[%d][%s-%s@60m][? %s ?]' % (series_sharp,\n",
    "                                                           harpnum,\n",
    "                                                        start_date,\n",
    "                                                        end_date,\n",
    "                                                   conditions),\n",
    "                       key=ids+sharps)\n",
    "\n",
    "        # if there are no data at this time, quit\n",
    "        if len(data_sharp) == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  harpnum)\n",
    "            continue\n",
    "\n",
    "        # query jsoc database for lorentz data\n",
    "        data_lorentz = c.query('%s[%d][%s-%s@60m][? %s ?]' % (series_lorentz,harpnum,\n",
    "                                                        start_date,\n",
    "                                                        end_date,\n",
    "                                                   conditions_lor),\n",
    "                       key=lorentzs)\n",
    "\n",
    "                # if there are no data at this time, quit\n",
    "        if len(data_lorentz) == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  harpnum)\n",
    "            continue\n",
    "\n",
    "        #concat the tables\n",
    "        data = pd.concat([data_sharp, data_lorentz], axis=1)\n",
    "\n",
    "        # check to see if the active region is too close to the limb\n",
    "        # we can compute the latitude of an active region in stonyhurst coordinates as follows:\n",
    "        # longitude_stonyhurst = CRVAL1 - CRLN_OBS\n",
    "        # for this we have to query the CEA series (but above we queried the other series as the CEA series does not have CODEVER5 in it)\n",
    "        data = data[np.abs(data['LON_FWT']) < 70.0]\n",
    "\n",
    "        # convert tai string to date time\n",
    "        data['T_REC'] = data['T_REC'].apply(parse_tai_string)\n",
    "\n",
    "        print('accept NOAA Active Region number', str(int(\n",
    "            listofactiveregions[i])), 'and HARPNUM', harpnum)\n",
    "\n",
    "        # Append to larger dataset\n",
    "        data_jsoc = pd.concat([data_jsoc, data], ignore_index=True)\n",
    "        # append to csv\n",
    "        outfile = '../Data/SHARP/jsoc_data.csv'\n",
    "        data.to_csv(outfile, mode='a', header=not os.path.exists(outfile),\n",
    "                    index=False)\n",
    "\n",
    "    return data_jsoc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call the function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "if os.path.exists('../Data/SHARP/jsoc_data.csv'):\n",
    "    data_jsoc = pd.read_csv('../Data/SHARP/jsoc_data.csv')\n",
    "else:\n",
    "    data_jsoc = get_the_jsoc_data(len(listofactiveregions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Match data with flares\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# extra cleanup\n",
    "data_jsoc = data_jsoc.drop(columns=['CRVAL1', 'CRVAL2', 'CRLN_OBS',\n",
    "                                    'CRLT_OBS'])\n",
    "data_jsoc['T_REC'] = pd.to_datetime(data_jsoc['T_REC'])\n",
    "data_jsoc = data_jsoc.sort_values(by=['T_REC', 'NOAA_AR']).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We take the closest peak time and previous peak time, get all the values\n",
    "between\n",
    "those times on the ```data_jsoc``` according to the class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5.4\n",
      "M6.6\n",
      "X2.2\n",
      "M6.6\n",
      "M5.3\n",
      "X1.5\n",
      "M9.3\n",
      "M6.0\n",
      "M9.3\n",
      "X6.9\n",
      "M5.3\n",
      "X2.1\n",
      "X1.8\n",
      "M6.7\n",
      "X1.4\n",
      "X1.9\n",
      "M7.1\n",
      "M5.8\n",
      "M7.4\n",
      "X1.9\n",
      "M8.7\n",
      "X1.7\n",
      "X1.1\n",
      "X5.4\n",
      "M6.3\n",
      "M8.4\n",
      "M7.9\n",
      "M5.7\n",
      "M5.1\n",
      "M5.6\n",
      "M5.3\n",
      "M6.1\n",
      "X1.1\n",
      "M6.9\n",
      "M6.1\n",
      "M5.5\n",
      "M9.0\n",
      "M5.0\n",
      "X1.8\n",
      "M6.0\n",
      "M6.5\n",
      "M5.7\n",
      "X1.7\n",
      "X2.8\n",
      "X3.2\n",
      "X1.2\n",
      "M5.0\n",
      "M5.9\n",
      "M9.3\n",
      "X1.7\n",
      "X2.1\n",
      "X1.0\n",
      "M5.1\n",
      "X2.3\n",
      "M6.3\n",
      "M5.0\n",
      "X3.3\n",
      "X1.1\n",
      "X1.1\n",
      "X1.0\n",
      "M6.4\n",
      "M9.9\n",
      "M7.2\n",
      "X1.2\n",
      "M6.6\n",
      "M5.2\n",
      "X4.9\n",
      "M9.3\n",
      "X1.0\n",
      "M6.5\n",
      "M7.3\n",
      "X2.2\n",
      "X1.5\n",
      "X1.0\n",
      "M6.5\n",
      "X1.1\n",
      "M8.7\n",
      "X1.6\n",
      "X3.1\n",
      "X1.0\n",
      "X2.0\n",
      "M7.1\n",
      "M6.7\n",
      "X2.0\n",
      "M6.6\n",
      "M6.5\n",
      "M7.9\n",
      "M5.4\n",
      "X1.6\n",
      "M5.7\n",
      "M6.1\n",
      "M8.7\n",
      "M6.9\n",
      "X1.8\n",
      "M5.6\n",
      "M8.2\n",
      "M9.2\n",
      "M5.8\n",
      "M5.1\n",
      "X2.1\n",
      "X2.7\n",
      "M6.5\n",
      "M7.9\n",
      "M5.6\n",
      "M7.6\n",
      "M5.5\n",
      "M6.7\n",
      "M5.3\n",
      "M5.7\n",
      "M5.8\n",
      "M5.5\n",
      "X2.2\n",
      "X9.3\n",
      "M7.3\n",
      "X1.3\n",
      "M8.1\n",
      "X8.2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(listofresults)):\n",
    "    start_time = listofresults['start_time'].iloc[i]\n",
    "    peak_time = listofresults['peak_time'].iloc[i]\n",
    "    noaa_num = listofresults['noaa_active_region'].iloc[i]\n",
    "    goes_class = listofresults['goes_class'].iloc[i]\n",
    "    if not noaa_num in data_jsoc['NOAA_AR'].unique():\n",
    "        continue\n",
    "    # get current noaa's data\n",
    "    df = data_jsoc[data_jsoc['NOAA_AR'] == noaa_num]\n",
    "    ar_start_time = df['T_REC'].iloc[0]\n",
    "\n",
    "    previous_peak_time = ar_start_time if i == 1 else \\\n",
    "        listofresults['peak_time'].iloc[i-1]\n",
    "\n",
    "    # create bolean mask\n",
    "    mask = ( (df['T_REC'] > previous_peak_time) &\n",
    "             (df['T_REC'] < peak_time))\n",
    "    current_flare = df.loc[mask]\n",
    "    data_jsoc.loc[data_jsoc.index[current_flare.index], 'flare'] = goes_class\n",
    "\n",
    "    # label positive for minimum_class_label before peak time, else label\n",
    "    # negative\n",
    "    if any(c in goes_class for c in minimum_class_label):\n",
    "        time_before = peak_time - timedelta(hours=timedelayvariable)\n",
    "        # get samples between peak and 24h before\n",
    "        mask = ( (df['T_REC'] > time_before) &\n",
    "             (df['T_REC'] < peak_time))\n",
    "        time_before_flare_df = df.loc[mask]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data_jsoc['flare'] = data_jsoc['flare'].replace(np.nan, 'N')\n",
    "# temp = data_jsoc[data_jsoc['flare'].str.contains('M')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label Positive class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label Negative class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step3: Generate history data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Calculate Decay values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Assign postive and negative classes?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
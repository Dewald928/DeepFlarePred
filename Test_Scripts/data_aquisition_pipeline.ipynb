{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Solar Flare prediction dataset pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a replication of the dataset used by [Liu et al.](https://web.njit.edu/~wangj/LSTMpredict/).\n",
    "\n",
    "The data comes from 2 sources:\n",
    "\n",
    "1. Flare data from the GOES flare catalog at NOAA, which can be accessed with the sunpy.instr.goes.get_event_list() function.\n",
    " This tells us if an active region produced a flare or not.\n",
    "2. Active region data from the Solar Dynamics Observatory's Heliosesmic and Magnetic Imager instrument, which can be accessed from the JSOC database via a JSON API.\n",
    "This gives us the features characterizing each active region.\n",
    "\n",
    "We ascribe each Active Region (AR) to one of two classes:\n",
    "\n",
    "1. The positive class contains flaring active regions that will produce\n",
    "flare >M5.0 in the next 24hours.\n",
    "2. The negative class contains flaring active regions that will **not**\n",
    "produce flare >M5.0 in the next 24hours.\n",
    "\n",
    "First, some imports.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "from datetime import datetime as dt_obj\n",
    "from datetime import timedelta\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sunpy.time import TimeRange\n",
    "from sunpy.net import hek\n",
    "from astropy.time import Time\n",
    "import sunpy.instr.goes\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import os\n",
    "import drms\n",
    "pd.set_option('display.max_rows', 100)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Get flare list\n",
    "\n",
    "We get the entire GOES flare catalog at NOAA.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbed all the GOES data; there are 11986 events.\n"
     ]
    }
   ],
   "source": [
    "# Grab all the data from the GOES database\n",
    "t_start = \"2010-05-01\"\n",
    "t_end = \"2018-05-11\"\n",
    "time_range = TimeRange(t_start, t_end)\n",
    "if os.path.exists(\"../Data/GOES/all_flares_list.csv\"):\n",
    "    listofresults = pd.read_csv('../Data/GOES/all_flares_list.csv').drop\\\n",
    "        (columns=\"Unnamed: 0\")\n",
    "else:\n",
    "    listofresults = sunpy.instr.goes.get_goes_event_list(time_range, 'B1')\n",
    "    # Remove all events without NOAA number\n",
    "    listofresults = listofresults[listofresults['noaa_active_region'] != 0]\n",
    "    # save to csv\n",
    "    pd.DataFrame(listofresults).to_csv('../Data/GOES/all_flares_list.csv')\n",
    "    listofresults = pd.DataFrame(listofresults)\n",
    "\n",
    "listofresults = listofresults.sort_values(by=['noaa_active_region',\n",
    "                                            'start_time']).reset_index(drop=True)\n",
    "print('Grabbed all the GOES data; there are', len(listofresults), 'events.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the ```times``` in the ```listofresults``` dataframe from a string\n",
    "into a datetime object:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def parse_tai_string(tstr):\n",
    "    year = int(tstr[:4])\n",
    "    month = int(tstr[5:7])\n",
    "    day = int(tstr[8:10])\n",
    "    hour = int(tstr[11:13])\n",
    "    minute = int(tstr[14:16])\n",
    "    return dt_obj(year, month, day, hour, minute)\n",
    "\n",
    "listofresults['start_time'] = listofresults['start_time'].apply(parse_tai_string)\n",
    "listofresults['peak_time'] = listofresults['peak_time'].apply(parse_tai_string)\n",
    "listofresults['end_time'] = listofresults['end_time'].apply(parse_tai_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's query the JSOC database to see if there are active region parameters at the time of the flare.\n",
    "First read the following file to map NOAA active region numbers to HARPNUMs (a HARP, or an HMI Active Region Patch, is the preferred numbering system for the HMI active regions as they appear in the magnetic field data before NOAA observes them in white light):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "HARP_NOAA_list = pd.read_csv(\n",
    "    'http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt', sep=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's determine at which time we'd like to predict CMEs. In general,\n",
    "many people try to predict a flare either 24 or 48 hours before it happens.\n",
    "We can report both in this study by setting a variable called ```timedelayvariable```:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "timedelayvariable = 24"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Get SHARP data\n",
    "\n",
    "Now we can grab the SDO data from the JSOC database by executing the JSON queries.\n",
    "We are selecting data that satisfies several criteria:\n",
    "The data has to be [1] disambiguated with a version of the disambiguation module greater than 1.1,\n",
    " [2] taken while the orbital velocity of the spacecraft is less than 3500 m/s,\n",
    " [3] of a high quality, and\n",
    " [4] within 70 degrees of central meridian.\n",
    " If the data pass all these tests, they are stuffed into one dataframe\n",
    " ```data_jsoc```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We prepare the data tobe fed into function, with all the >M5.0 class flares\n",
    "labelled positive the ```timedelayvariable``` ahead of time.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "minimum_class_label = ['M5', 'M6', 'M7', 'M8', 'M9', 'X']\n",
    "listofactiveregions = list(listofresults['noaa_active_region'].unique())\n",
    "FL_GOESCLS_MAJOR_DICT = {'B': 1.0, 'C' : 2.0, 'M': 3.0, 'X': 4.0}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def get_the_jsoc_data(event_count):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_count: number of events\n",
    "                 int\n",
    "\n",
    "    t_rec:       list of times, one associated with each event in event_count\n",
    "                 list of strings in JSOC format ('%Y.%m.%d_%H:%M_TAI')\n",
    "\n",
    "    \"\"\"\n",
    "    from astropy.time import Time\n",
    "    start_date = drms.to_datetime(t_start).strftime('%Y.%m.%d_%H:%M_TAI')\n",
    "    end_date = drms.to_datetime(t_end).strftime('%Y.%m.%d_%H:%M_TAI')\n",
    "    series_sharp = 'hmi.sharp_cea_720s'\n",
    "    series_lorentz = 'cgem.lorentz'\n",
    "    ids = ['T_REC','NOAA_AR', 'HARPNUM', 'CRVAL1','CRVAL2', 'CRLN_OBS',\n",
    "           'CRLT_OBS', 'LAT_FWT', 'LON_FWT']\n",
    "    sharps = ['USFLUX', 'MEANGBT',\n",
    "              'MEANJZH', 'MEANPOT', 'SHRGT45',\n",
    "              'TOTUSJH', 'MEANGBH','MEANALP','MEANGAM','MEANGBZ','MEANJZD',\n",
    "              'TOTUSJZ','SAVNCPP', 'TOTPOT','MEANSHR','AREA_ACR','R_VALUE',\n",
    "              'ABSNJZH']\n",
    "    lorentzs = ['TOTBSQ', 'TOTFX','TOTFY','TOTFZ','EPSX','EPSY','EPSZ']\n",
    "    conditions = '(CODEVER7 !~ \"1.1\") and (abs(OBS_VR)< 3500) and (QUALITY<65536)'\n",
    "    conditions_lor = '(abs(OBS_VR)< 3500) and (QUALITY<65536)'\n",
    "    c = drms.Client()\n",
    "    data_jsoc = pd.DataFrame()\n",
    "\n",
    "    # for earch active region\n",
    "    for i in range(event_count):\n",
    "\n",
    "        print(\"=====\", i, \"=====\")\n",
    "        # next match NOAA_ARS to HARPNUM\n",
    "        idx = HARP_NOAA_list[HARP_NOAA_list['NOAA_ARS'].str.contains(\n",
    "            str(int(listofactiveregions[i])))]\n",
    "\n",
    "        # if there's no HARPNUM, quit\n",
    "        if (idx.empty == True):\n",
    "            print('skip: there are no matching HARPNUMs for',\n",
    "                  str(int(listofactiveregions[i])))\n",
    "            continue\n",
    "\n",
    "        harpnum = idx.HARPNUM.values[0]\n",
    "        # query jsoc database for sharp data\n",
    "        data_sharp = c.query('%s[%d][%s-%s@60m][? %s ?]' % (series_sharp,\n",
    "                                                           harpnum,\n",
    "                                                        start_date,\n",
    "                                                        end_date,\n",
    "                                                   conditions),\n",
    "                       key=ids+sharps)\n",
    "\n",
    "        # if there are no data at this time, quit\n",
    "        if len(data_sharp) == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  harpnum)\n",
    "            continue\n",
    "\n",
    "        # query jsoc database for lorentz data\n",
    "        data_lorentz = c.query('%s[%d][%s-%s@60m][? %s ?]' % (series_lorentz,harpnum,\n",
    "                                                        start_date,\n",
    "                                                        end_date,\n",
    "                                                   conditions_lor),\n",
    "                       key=lorentzs)\n",
    "\n",
    "                # if there are no data at this time, quit\n",
    "        if len(data_lorentz) == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  harpnum)\n",
    "            continue\n",
    "\n",
    "        #concat the tables\n",
    "        data = pd.concat([data_sharp, data_lorentz], axis=1)\n",
    "\n",
    "        # check to see if the active region is too close to the limb\n",
    "        # we can compute the latitude of an active region in stonyhurst coordinates as follows:\n",
    "        # longitude_stonyhurst = CRVAL1 - CRLN_OBS\n",
    "        # for this we have to query the CEA series (but above we queried the other series as the CEA series does not have CODEVER5 in it)\n",
    "        data = data[np.abs(data['LON_FWT']) < 70.0]\n",
    "\n",
    "        # convert tai string to date time\n",
    "        data['T_REC'] = data['T_REC'].apply(parse_tai_string)\n",
    "\n",
    "        print('accept NOAA Active Region number', str(int(\n",
    "            listofactiveregions[i])), 'and HARPNUM', harpnum)\n",
    "\n",
    "        # Append to larger dataset\n",
    "        data_jsoc = pd.concat([data_jsoc, data], ignore_index=True)\n",
    "        # append to csv\n",
    "        outfile = '../Data/SHARP/jsoc_data.csv'\n",
    "        data.to_csv(outfile, mode='a', header=not os.path.exists(outfile),\n",
    "                    index=False)\n",
    "\n",
    "    return data_jsoc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call the function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "if os.path.exists('../Data/SHARP/jsoc_data.csv'):\n",
    "    data_jsoc = pd.read_csv('../Data/SHARP/jsoc_data.csv')\n",
    "else:\n",
    "    data_jsoc = get_the_jsoc_data(len(listofactiveregions))\n",
    "# data_jsoc = get_the_jsoc_data(len(listofactiveregions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Match data with flares"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Flare to flux function for determining the Xmax1d data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def flare_to_flux(flare_class):\n",
    "    flux = 1e-7\n",
    "    if flare_class == 'N':\n",
    "        flux = 1e-7\n",
    "    else:\n",
    "        class_label = flare_class[0]\n",
    "        class_mag = float(flare_class[1:])\n",
    "        if class_label=='B':\n",
    "            flux = 1e-7 * class_mag\n",
    "        if class_label=='C':\n",
    "            flux = 1e-6 * class_mag\n",
    "        if class_label=='M':\n",
    "            flux = 1e-5 * class_mag\n",
    "        if class_label=='X':\n",
    "            flux = 1e-4 * class_mag\n",
    "    return flux\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# extra cleanup\n",
    "data_jsoc = data_jsoc.drop(columns=['CRVAL1', 'CRVAL2', 'CRLN_OBS',\n",
    "                                    'CRLT_OBS'])\n",
    "data_jsoc['T_REC'] = pd.to_datetime(data_jsoc['T_REC'])\n",
    "data_jsoc = data_jsoc.sort_values(by=['HARPNUM', 'T_REC']).reset_index\\\n",
    "    (drop=True)\n",
    "# prepare columns\n",
    "data_jsoc.insert(0, 'flare', 'N')\n",
    "data_jsoc.insert(0, 'label', np.nan)\n",
    "l = ['Bhis', 'Chis', 'Mhis', 'Xhis', 'Bhis1d', 'Chis1d', 'Mhis1d',\n",
    "           'Xhis1d']\n",
    "d = dict.fromkeys(l,0)\n",
    "data_jsoc = data_jsoc.assign(**d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We take the closest peak time and previous peak time, get all the values\n",
    "between and classify the class according to\n",
    "those times on the ```data_jsoc```. We also count the flare per AR."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def label_data():\n",
    "    # for the current flare\n",
    "    for i in range(listofresults.shape[0]-1):\n",
    "        start_time = listofresults['start_time'].iloc[i]\n",
    "        peak_time = listofresults['peak_time'].iloc[i]\n",
    "        next_peak_time = listofresults['peak_time'].iloc[i+1]\n",
    "        noaa_num = listofresults['noaa_active_region'].iloc[i]\n",
    "        noaa_num_previous = noaa_num if i==0 else noaa_num_previous\n",
    "        goes_class = listofresults['goes_class'].iloc[i]\n",
    "        goes_flux = flare_to_flux(goes_class)\n",
    "        if not noaa_num in data_jsoc['NOAA_AR'].unique():\n",
    "            continue\n",
    "\n",
    "        # if a new AR, restart count\n",
    "        if noaa_num_previous != noaa_num:\n",
    "            Bcount, Ccount, Mcount, Xcount = 0,0,0,0\n",
    "            noaa_num_previous = noaa_num\n",
    "\n",
    "        # get current noaa's data\n",
    "        df = data_jsoc[data_jsoc['NOAA_AR'] == noaa_num]\n",
    "        ar_start_time = df['T_REC'].iloc[0]\n",
    "\n",
    "        previous_peak_time = ar_start_time if \\\n",
    "            listofresults['noaa_active_region'].iloc[i-1] != noaa_num else \\\n",
    "            listofresults['peak_time'].iloc[i-1]\n",
    "\n",
    "        # create bolean mask and label flare class\n",
    "        mask = ( (df['T_REC'] > previous_peak_time) &\n",
    "                 (df['T_REC'] < peak_time))\n",
    "        current_flare = df.loc[mask]\n",
    "        data_jsoc.loc[data_jsoc.index[current_flare.index], 'flare'] = goes_class\n",
    "\n",
    "        # get values after the peak\n",
    "        tdelta = df['T_REC'] - peak_time\n",
    "        tdelta_h = tdelta / np.timedelta64(1, \"h\")\n",
    "        tdelta_mask = tdelta_h > 0\n",
    "\n",
    "        # add previous flare count\n",
    "        after_flare = df.loc[tdelta_mask]\n",
    "        data_jsoc.loc[data_jsoc.index[after_flare.index], '{}his'.format\n",
    "        (goes_class[0])] += 1\n",
    "\n",
    "        # 1day history\n",
    "        time_after = peak_time + timedelta(hours=24)\n",
    "        mask = ((df['T_REC'] < time_after) &\n",
    "                 (df['T_REC'] > peak_time))\n",
    "        history_day_df = df.loc[mask]\n",
    "        data_jsoc.loc[data_jsoc.index[history_day_df.index], '{}his1d'\n",
    "            .format(goes_class[0])] += 1\n",
    "        data_jsoc.loc[data_jsoc.index[history_day_df.index], 'Xmax1d'] = goes_flux\n",
    "\n",
    "\n",
    "        # label positive for minimum_class_label before peak time, else label\n",
    "        # negative\n",
    "        if any(c in goes_class for c in minimum_class_label):\n",
    "            time_before = peak_time - timedelta(hours=timedelayvariable)\n",
    "            # get samples between peak and 24h before, label as positive\n",
    "            mask = ((df['T_REC'] > time_before) &\n",
    "                 (df['T_REC'] < peak_time))\n",
    "            time_before_flare_df = df.loc[mask]\n",
    "            data_jsoc.loc[data_jsoc.index[time_before_flare_df.index], 'label'] = \\\n",
    "                'Positive'\n",
    "\n",
    "    # label other examples as negative\n",
    "    data_jsoc['flare'] = data_jsoc['flare'].replace(np.nan, 'N')\n",
    "    data_jsoc['label'] = data_jsoc['label'].replace(np.nan, 'Negative')\n",
    "    return data_jsoc\n",
    "\n",
    "# data_jsoc_labelled = label_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We call the labelling function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# if the file isn't there, generate data\n",
    "filename_data_jsoc_labelled = '../Data/SHARP/jsoc_data_labelled.csv'\n",
    "if os.path.exists(filename_data_jsoc_labelled):\n",
    "    data_jsoc_labelled = pd.read_csv(filename_data_jsoc_labelled)\n",
    "else:\n",
    "    data_jsoc_labelled = label_data()\n",
    "    data_jsoc_labelled.to_csv(filename_data_jsoc_labelled, header=not os.path.exists(filename_data_jsoc_labelled),\n",
    "            index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Calculate Decay values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the peak of a flare in an AR, the respective decay values are\n",
    "calculated. with a decay rate of 12 hours.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def get_decay():\n",
    "    data_jsoc_labelled['T_REC'] = pd.to_datetime(data_jsoc_labelled['T_REC'])\n",
    "    data_jsoc_labelled['Bdec'] = 0.0\n",
    "    data_jsoc_labelled['Cdec'] = 0.0\n",
    "    data_jsoc_labelled['Mdec'] = 0.0\n",
    "    data_jsoc_labelled['Xdec'] = 0.0\n",
    "    data_jsoc_labelled['Edec'] = 0.0\n",
    "    data_jsoc_labelled['logEdec'] = 0.0\n",
    "    decay_tau_min = 12\n",
    "\n",
    "    for i in range(listofresults.shape[0]-1):\n",
    "        start_time = listofresults['start_time'].iloc[i]\n",
    "        peak_time = listofresults['peak_time'].iloc[i]\n",
    "        next_peak_time = listofresults['peak_time'].iloc[i+1]\n",
    "        noaa_num = listofresults['noaa_active_region'].iloc[i]\n",
    "        goes_class = listofresults['goes_class'].iloc[i]\n",
    "        goescls_major = goes_class[0]\n",
    "        if not noaa_num in data_jsoc['NOAA_AR'].unique():\n",
    "            continue\n",
    "\n",
    "        # get current noaa's data\n",
    "        df = data_jsoc_labelled[data_jsoc['NOAA_AR'] == noaa_num]\n",
    "        ar_start_time = df['T_REC'].iloc[0]\n",
    "\n",
    "        # get values after the peak\n",
    "        tdelta = df['T_REC'] - peak_time\n",
    "        tdelta_h = tdelta / np.timedelta64(1, \"h\")\n",
    "        tdelta_mask = tdelta_h < 0\n",
    "        after_flare = df.loc[tdelta_mask]\n",
    "        # data_jsoc_labelled.loc[data_jsoc_labelled.index[after_flare.index], '{}his'.format\n",
    "        # (goes_class[0])] += 1\n",
    "\n",
    "        # Get the decay values\n",
    "        tdelta_exp = np.exp(-tdelta_h/ decay_tau_min)\n",
    "        tdelta_exp[tdelta_mask] = 0.0\n",
    "\n",
    "        df.loc[:,'{}dec'.format(goescls_major)] += tdelta_exp\n",
    "\n",
    "        # get the energy decay values\n",
    "        fl_energy_peak = 10**FL_GOESCLS_MAJOR_DICT[goescls_major] + \\\n",
    "                         float(goes_class[1:])\n",
    "\n",
    "        df.loc[:,'Edec'] += tdelta_exp * fl_energy_peak\n",
    "\n",
    "        df.loc[:,'logEdec'] += tdelta_exp * \\\n",
    "                                   FL_GOESCLS_MAJOR_DICT[goescls_major]\n",
    "\n",
    "        data_jsoc_labelled.update(df)\n",
    "    return data_jsoc_labelled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "filename_data_raw = '../Data/SHARP/raw_data.csv'\n",
    "if os.path.exists(filename_data_raw):\n",
    "    data_raw = pd.read_csv(filename_data_raw)\n",
    "else:\n",
    "    data_raw = get_decay()\n",
    "    data_raw.to_csv(filename_data_raw, header=not os.path.exists(filename_data_raw),\n",
    "            index=False)\n",
    "#todo check header\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the features have different units and scales, we\n",
    "normalize the feature values as follows. For the 25 physical features, let\n",
    "$z_i^k$ denote the normalized value of the ith feature of the\n",
    "kth data sample. Then\n",
    "\n",
    "\\begin{align}\n",
    "z_{i}^{k}=\\frac{v_{i}^{k}-\\mu_{i}}{\\sigma_{i}}\n",
    "\\end{align}\n",
    "\n",
    "where $v_i^k$ is the original value of the ith feature of the kth data\n",
    "sample, $\\mu _i$ is the mean of the ith feature, and $\\sigma _i$ is the\n",
    "standard\n",
    "deviation\n",
    "of the ith feature. For the 15 flare history features, we have\n",
    "\n",
    "\\begin{align}\n",
    "z_{i}^{k}=\\frac{v_{i}^{k}}{\\max _{i}-\\min _{i}}\n",
    "\\end{align}\n",
    "\n",
    "where $max_i$ and $min_i$ are the maximum and minimum values of the ith\n",
    "feature,\n",
    "respectively.\n",
    "\n",
    "## Cleanup data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delete rows that have nan as in physical features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "sharps = ['USFLUX', 'MEANGBT', 'MEANJZH', 'MEANPOT', 'SHRGT45', 'TOTUSJH',\n",
    "          'MEANGBH', 'MEANALP', 'MEANGAM', 'MEANGBZ', 'MEANJZD', 'TOTUSJZ',\n",
    "          'SAVNCPP', 'TOTPOT', 'MEANSHR', 'AREA_ACR', 'R_VALUE',\n",
    "          'ABSNJZH']\n",
    "lorentz = ['TOTBSQ','TOTFX', 'TOTFY', 'TOTFZ', 'EPSX', 'EPSY', 'EPSZ']\n",
    "data_raw = data_raw.dropna(axis=0, subset=sharps+lorentz)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make history nans to 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "data_raw = data_raw.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change naming scheme to match Liu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "data_raw = data_raw.rename(columns={\"T_REC\": \"timestep\", \"NOAA_AR\": \"NOAA\",\n",
    "                                  \"HARPNUM\":\n",
    "    \"HARP\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the data\n",
    "First we need to split the data set into the training, validation and\n",
    "testing sets. The normalization is fitted on the training and\n",
    "validation set, and only transformed on the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "# start harp 3542\n",
    "idx_val_start = data_raw[data_raw['HARP'] == 3542].index[0]\n",
    "# end harp 4995\n",
    "idx_val_end = data_raw[data_raw['HARP'] == 4995].index[-1]\n",
    "training_set = data_raw.iloc[:idx_val_start-1,:]\n",
    "validation_set = data_raw.iloc[idx_val_start:idx_val_end,:]\n",
    "test_set = data_raw.iloc[idx_val_end+1:,:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Physical features normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "startfeature = 5\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler = StandardScaler()\n",
    "# fit on trainval\n",
    "physical_features_train = training_set.iloc[:, startfeature:startfeature+27]\n",
    "physical_features_val = validation_set.iloc[:, startfeature:startfeature+27]\n",
    "physical_features_test = test_set.iloc[:, startfeature:startfeature+27]\n",
    "standardscaler.fit(pd.concat([physical_features_train, physical_features_val]))\n",
    "new_physical_features_train = pd.DataFrame(standardscaler.transform\n",
    "                                           (physical_features_train),\n",
    "                                        index=physical_features_train.index,\n",
    "                                        columns=physical_features_train.columns)\n",
    "new_physical_features_val = pd.DataFrame(standardscaler.transform\n",
    "                                           (physical_features_val),\n",
    "                                        index=physical_features_val.index,\n",
    "                                        columns=physical_features_val.columns)\n",
    "new_physical_features_test = pd.DataFrame(standardscaler.transform\n",
    "                                          (physical_features_test),\n",
    "                                        index=physical_features_test.index,\n",
    "                                        columns=physical_features_test.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## History normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmaxscaler = MinMaxScaler()\n",
    "his_features_train = training_set.iloc[:, startfeature+28:]\n",
    "his_features_val = validation_set.iloc[:, startfeature+28:]\n",
    "his_features_test = test_set.iloc[:, startfeature+28:]\n",
    "minmaxscaler.fit(pd.concat([his_features_train, his_features_val]))\n",
    "new_his_features_train = pd.DataFrame(minmaxscaler.transform\n",
    "                                           (his_features_train),\n",
    "                                        index=his_features_train.index,\n",
    "                                        columns=his_features_train.columns)\n",
    "new_his_features_val = pd.DataFrame(minmaxscaler.transform\n",
    "                                           (his_features_val),\n",
    "                                        index=his_features_val.index,\n",
    "                                        columns=his_features_val.columns)\n",
    "new_his_features_test = pd.DataFrame(minmaxscaler.transform\n",
    "                                          (his_features_test),\n",
    "                                        index=his_features_test.index,\n",
    "                                        columns=his_features_test.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# make new normalized dataset\n",
    "normalized_train = pd.concat([data_raw.iloc[:idx_val_start-1,:startfeature],\n",
    "                             new_physical_features_train,\n",
    "                              new_his_features_train],\n",
    "                            axis=1)\n",
    "normalized_val = pd.concat([data_raw.iloc[idx_val_start:idx_val_end,:startfeature],\n",
    "                             new_physical_features_val, new_his_features_val],\n",
    "                            axis=1)\n",
    "normalized_test = pd.concat([data_raw.iloc[idx_val_end+1:,:startfeature],\n",
    "                             new_physical_features_test,\n",
    "                             new_his_features_test],\n",
    "                            axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save to csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "filename_krynauw = '../Data/Krynauw/'\n",
    "training_set.to_csv(filename_krynauw + 'training.csv', index=False)\n",
    "validation_set.to_csv(filename_krynauw + 'validation.csv', index=False)\n",
    "test_set.to_csv(filename_krynauw + 'testing.csv', index=False)\n",
    "\n",
    "normalized_train.to_csv(filename_krynauw + 'normalized_training.csv', index=False)\n",
    "normalized_val.to_csv(filename_krynauw + 'normalized_validation.csv', index=False)\n",
    "normalized_test.to_csv(filename_krynauw + 'normalized_testing.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}